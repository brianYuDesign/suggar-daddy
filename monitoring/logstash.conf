input {
  # Docker logs input
  tcp {
    host => "0.0.0.0"
    port => 5000
    codec => json
    type => "docker"
  }

  # Syslog input
  udp {
    host => "0.0.0.0"
    port => 514
    type => "syslog"
  }

  # HTTP input for application logs
  http {
    host => "0.0.0.0"
    port => 8080
    codec => json
    type => "app-logs"
  }

  # File input for application logs
  file {
    path => "/var/log/sugar-daddy/app.log"
    start_position => "beginning"
    codec => json
    type => "app-logs"
    tags => ["recommendation-service"]
  }

  # File input for content-streaming logs
  file {
    path => "/var/log/sugar-daddy/content-streaming.log"
    start_position => "beginning"
    codec => json
    type => "app-logs"
    tags => ["content-streaming-service"]
  }

  # File input for auth service logs
  file {
    path => "/var/log/sugar-daddy/auth.log"
    start_position => "beginning"
    codec => json
    type => "app-logs"
    tags => ["auth-service"]
  }

  # File input for postgres logs
  file {
    path => "/var/log/sugar-daddy/postgres.log"
    start_position => "beginning"
    type => "postgres-logs"
    tags => ["postgres"]
  }

  # File input for redis logs
  file {
    path => "/var/log/sugar-daddy/redis.log"
    start_position => "beginning"
    type => "redis-logs"
    tags => ["redis"]
  }

  # Beats input (Filebeat, Metricbeat)
  beats {
    port => 5044
    type => "beats"
  }
}

filter {
  # Parse JSON logs
  if [@metadata][type] == "app-logs" or [@metadata][type] == "docker" {
    json {
      source => "message"
      target => "parsed"
    }
  }

  # Parse syslog
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGLINE}" }
    }
  }

  # Extract request information from app logs
  if [type] == "app-logs" {
    mutate {
      add_field => { "[@metadata][index_name]" => "logs-%{+YYYY.MM.dd}" }
    }

    # Extract API metrics
    if [message] =~ /method/ {
      grok {
        match => { "message" => "%{GREEDYDATA:request_body}.*method\":\"(?<http_method>[A-Z]+)\"" }
        match => { "message" => ".*path\":\"(?<http_path>[^\"]+)\"" }
        match => { "message" => ".*status\":(?<http_status>\d+)" }
        match => { "message" => ".*duration\":(?<response_time>\d+(?:\.\d+)?)" }
      }
    }

    # Parse timestamp
    if [timestamp] {
      date {
        match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss"]
        target => "@timestamp"
      }
    }

    # Add service name
    mutate {
      add_field => { "service" => "%{[tags][0]}" }
      add_field => { "environment" => "development" }
    }
  }

  # Parse PostgreSQL logs
  if [type] == "postgres-logs" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:process}\] %{LOGLEVEL:level}:%{GREEDYDATA:message}"
      }
    }

    # Extract query duration
    if [message] =~ /duration/ {
      grok {
        match => { "message" => "duration: (?<query_duration>[\d.]+) ms" }
      }
    }
  }

  # Parse Redis logs
  if [type] == "redis-logs" {
    grok {
      match => {
        "message" => "%{INT:pid}:M %{MONTHDAY} %{MONTH} %{TIME} \* %{LOGLEVEL:level} %{GREEDYDATA:message}"
      }
    }
  }

  # Add geo information (optional)
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }

  # Add user agent parsing
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "user_agent_parsed"
    }
  }

  # Remove unnecessary fields
  mutate {
    remove_field => ["host", "[host]"]
  }
}

output {
  # Elasticsearch output
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_name]}"
    action => "index"
    doc_type => "_doc"
    # 如果需要認證
    # user => "${ELASTICSEARCH_USER}"
    # password => "${ELASTICSEARCH_PASSWORD}"
  }

  # Stdout for debugging (development only)
  if [@metadata][pipeline] == "test" {
    stdout {
      codec => json
    }
  }

  # Backup to file
  file {
    path => "/var/log/logstash/backup-%{+YYYY.MM.dd-HH}.log"
    codec => json
    flush_interval => 0
  }
}
