# DB Writer Service

## ğŸ“– ç°¡ä»‹

DB Writer Service æ˜¯æ•´å€‹ç³»çµ±ä¸­**å”¯ä¸€**è² è²¬å¯«å…¥ PostgreSQL è³‡æ–™åº«çš„æœå‹™ï¼Œæ¡ç”¨ CQRS (Command Query Responsibility Segregation) å’Œ Event Sourcing æ¨¡å¼ï¼Œç¢ºä¿è³‡æ–™ä¸€è‡´æ€§å’Œç³»çµ±è§£è€¦ã€‚

## ğŸ¯ è·è²¬èªªæ˜

- **äº‹ä»¶æ¶ˆè²»**: ç›£è½ Kafka æ‰€æœ‰å¯«å…¥ç›¸é—œäº‹ä»¶
- **è³‡æ–™æŒä¹…åŒ–**: å°‡äº‹ä»¶è½‰æ›ç‚ºè³‡æ–™åº«å¯«å…¥æ“ä½œ
- **äº‹å‹™ç®¡ç†**: ç¢ºä¿è³‡æ–™åº«å¯«å…¥çš„ ACID ç‰¹æ€§
- **å†ªç­‰æ€§**: è™•ç†é‡è¤‡äº‹ä»¶ï¼Œé¿å…è³‡æ–™é‡è¤‡
- **éŒ¯èª¤é‡è©¦**: å¤±æ•—äº‹ä»¶çš„é‡è©¦æ©Ÿåˆ¶
- **è³‡æ–™å®Œæ•´æ€§**: é©—è­‰è³‡æ–™å®Œæ•´æ€§å’Œé—œè¯æ€§

## ğŸš€ ç«¯å£å’Œè·¯ç”±

- **ç«¯å£**: `3010`ï¼ˆåƒ…ç”¨æ–¼å¥åº·æª¢æŸ¥ï¼Œç„¡ HTTP APIï¼‰
- **æ¨¡å¼**: Kafka Consumerï¼ˆäº‹ä»¶é©…å‹•ï¼‰

## ğŸ› ï¸ æŠ€è¡“æ£§

- **æ¡†æ¶**: NestJS
- **èªè¨€**: TypeScript
- **ORM**: TypeORM
- **è³‡æ–™åº«**: PostgreSQL
- **è¨Šæ¯ä½‡åˆ—**: Kafka Consumer
- **å¿«å–**: Redisï¼ˆç”¨æ–¼å†ªç­‰æ€§æª¢æŸ¥ï¼‰

## âš™ï¸ ç’°å¢ƒè®Šæ•¸

```bash
# æœå‹™ç«¯å£ï¼ˆåƒ…å¥åº·æª¢æŸ¥ï¼‰
DB_WRITER_SERVICE_PORT=3010
PORT=3010

# PostgreSQL è¨­å®š
DB_HOST=localhost
DB_PORT=5432
DB_USERNAME=postgres
DB_PASSWORD=postgres
DB_DATABASE=suggar_daddy
DB_POOL_SIZE=20                    # é€£ç·šæ± å¤§å°
DB_CONNECTION_TIMEOUT=5000         # é€£ç·šè¶…æ™‚ï¼ˆæ¯«ç§’ï¼‰

# Kafka è¨­å®š
KAFKA_BROKERS=localhost:9092
KAFKA_CLIENT_ID=db-writer-service
KAFKA_GROUP_ID=db-writer-group
KAFKA_AUTO_COMMIT=false            # æ‰‹å‹•æäº¤ç¢ºä¿è³‡æ–™å¯«å…¥æˆåŠŸ

# Redis è¨­å®šï¼ˆå†ªç­‰æ€§æª¢æŸ¥ï¼‰
REDIS_HOST=localhost
REDIS_PORT=6379
IDEMPOTENCY_TTL=86400              # äº‹ä»¶ ID å¿«å– 24 å°æ™‚

# é‡è©¦è¨­å®š
MAX_RETRY_ATTEMPTS=3
RETRY_DELAY_MS=1000
EXPONENTIAL_BACKOFF=true

# æ‰¹æ¬¡è™•ç†
BATCH_SIZE=10                      # æ‰¹æ¬¡è™•ç†äº‹ä»¶æ•¸é‡
BATCH_TIMEOUT_MS=1000              # æ‰¹æ¬¡è¶…æ™‚
```

## ğŸ’» æœ¬åœ°é–‹ç™¼æŒ‡ä»¤

```bash
# å•Ÿå‹•é–‹ç™¼ä¼ºæœå™¨
nx serve db-writer-service

# å»ºç½®
nx build db-writer-service

# åŸ·è¡Œæ¸¬è©¦
nx test db-writer-service

# Lint æª¢æŸ¥
nx lint db-writer-service

# è³‡æ–™åº«é·ç§»
npm run typeorm migration:run
npm run typeorm migration:generate -- -n MigrationName
```

## ğŸ“Š ç›£è½çš„ Kafka ä¸»é¡Œ

### ç”¨æˆ¶ç›¸é—œ (user.*)

```typescript
// user.created
{
  eventId: string;
  eventType: 'user.created';
  timestamp: Date;
  data: {
    userId: string;
    username: string;
    email: string;
    role: UserRole;
    // ...
  }
}

// user.updated
{
  eventId: string;
  eventType: 'user.updated';
  timestamp: Date;
  data: {
    userId: string;
    updates: {
      displayName?: string;
      bio?: string;
      // ...
    }
  }
}

// user.deleted
// user.followed
// user.unfollowed
// user.blocked
// user.unblocked
```

### å…§å®¹ç›¸é—œ (content.*)

```typescript
// content.post.created
{
  eventId: string;
  eventType: 'content.post.created';
  timestamp: Date;
  data: {
    postId: string;
    authorId: string;
    content: string;
    mediaUrls: string[];
    visibility: string;
    // ...
  }
}

// content.post.updated
// content.post.deleted
// content.post.liked
// content.comment.created
// content.story.created
// content.video.created
```

### æ”¯ä»˜ç›¸é—œ (payment.*)

```typescript
// payment.completed
{
  eventId: string;
  eventType: 'payment.completed';
  timestamp: Date;
  data: {
    transactionId: string;
    userId: string;
    amount: number;
    type: 'TIP' | 'PURCHASE' | 'SUBSCRIPTION';
    // ...
  }
}

// payment.tip.created
// payment.refunded
// wallet.updated
// withdrawal.requested
```

### è¨‚é–±ç›¸é—œ (subscription.*)

```typescript
// subscription.created
{
  eventId: string;
  eventType: 'subscription.created';
  timestamp: Date;
  data: {
    subscriptionId: string;
    userId: string;
    tierId: string;
    status: string;
    // ...
  }
}

// subscription.renewed
// subscription.canceled
// subscription.expired
```

### æŠ€èƒ½ç›¸é—œ (skill.*)

```typescript
// skill.created
// skill.updated
// skill.user.added
// skill.user.removed
```

### é€šçŸ¥å’Œè¨Šæ¯ (notification.*, message.*)

```typescript
// notification.created
// message.created
// message.read
```

## ğŸ”„ è³‡æ–™æµç¨‹

### äº‹ä»¶è™•ç†æµç¨‹

```mermaid
sequenceDiagram
    participant Kafka
    participant Writer as DB Writer Service
    participant Redis
    participant PG as PostgreSQL

    Kafka->>Writer: æ¶ˆè²»äº‹ä»¶
    Writer->>Redis: æª¢æŸ¥äº‹ä»¶ ID (å†ªç­‰æ€§)
    
    alt äº‹ä»¶å·²è™•ç†
        Writer->>Kafka: æäº¤ offset (è·³é)
    else æ–°äº‹ä»¶
        Writer->>PG: é–‹å§‹äº‹å‹™
        Writer->>PG: å¯«å…¥è³‡æ–™
        Writer->>Redis: å„²å­˜äº‹ä»¶ ID
        Writer->>PG: æäº¤äº‹å‹™
        Writer->>Kafka: æäº¤ offset
    end
    
    alt å¯«å…¥å¤±æ•—
        Writer->>Writer: é‡è©¦ (æœ€å¤š 3 æ¬¡)
        Writer->>Kafka: ä¸æäº¤ offset (é‡æ–°æ¶ˆè²»)
    end
```

### æ‰¹æ¬¡è™•ç†æµç¨‹

1. ç´¯ç©äº‹ä»¶åˆ°æ‰¹æ¬¡å¤§å° (é è¨­ 10)
2. æˆ–é”åˆ°è¶…æ™‚æ™‚é–“ (é è¨­ 1 ç§’)
3. æ‰¹æ¬¡é–‹å§‹äº‹å‹™
4. æ‰¹æ¬¡å¯«å…¥æ‰€æœ‰äº‹ä»¶
5. æ‰¹æ¬¡æäº¤äº‹å‹™å’Œ offset

## ğŸ›¡ï¸ å†ªç­‰æ€§ä¿è­‰

### æ–¹æ¡ˆ

ä½¿ç”¨ Redis å„²å­˜å·²è™•ç†çš„äº‹ä»¶ IDï¼š

```typescript
const eventKey = `processed:${eventId}`;
const exists = await redis.exists(eventKey);

if (exists) {
  // è·³éå·²è™•ç†çš„äº‹ä»¶
  return;
}

// è™•ç†äº‹ä»¶
await processEvent(event);

// æ¨™è¨˜ç‚ºå·²è™•ç†ï¼ˆ24 å°æ™‚éæœŸï¼‰
await redis.setex(eventKey, 86400, '1');
```

### ç‚ºä»€éº¼éœ€è¦å†ªç­‰æ€§

- Kafka å¯èƒ½é‡è¤‡æŠ•éäº‹ä»¶
- æœå‹™é‡å•Ÿæ™‚å¯èƒ½é‡æ–°æ¶ˆè²»æœªæäº¤çš„ offset
- ç¶²è·¯å•é¡Œå°è‡´çš„é‡è©¦

## ğŸ“ˆ æ•ˆèƒ½å„ªåŒ–

### æ‰¹æ¬¡å¯«å…¥

```typescript
// æ‰¹æ¬¡æ’å…¥å¤šç­†è³‡æ–™
await postRepository
  .createQueryBuilder()
  .insert()
  .values(posts)
  .execute();
```

### é€£ç·šæ± ç®¡ç†

```typescript
{
  type: 'postgres',
  poolSize: 20,
  extra: {
    max: 20,
    idleTimeoutMillis: 30000,
    connectionTimeoutMillis: 5000
  }
}
```

### ç´¢å¼•å„ªåŒ–

ç¢ºä¿æ‰€æœ‰æŸ¥è©¢æ¢ä»¶å’Œå¤–éµéƒ½æœ‰ç´¢å¼•ã€‚

## ğŸ”§ éŒ¯èª¤è™•ç†

### é‡è©¦æ©Ÿåˆ¶

```typescript
async function processEventWithRetry(event: Event, maxRetries = 3) {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      await processEvent(event);
      return; // æˆåŠŸ
    } catch (error) {
      if (attempt === maxRetries) {
        // æœ€å¾Œä¸€æ¬¡é‡è©¦å¤±æ•—ï¼Œç™¼é€åˆ° Dead Letter Queue
        await sendToDeadLetterQueue(event, error);
        throw error;
      }
      // æŒ‡æ•¸é€€é¿
      await sleep(1000 * Math.pow(2, attempt - 1));
    }
  }
}
```

### Dead Letter Queue (DLQ)

å¤±æ•—çš„äº‹ä»¶ç™¼é€åˆ° DLQ ä¸»é¡Œï¼š

- `db-writer.dlq`

å¯æ‰‹å‹•é‡æ–°è™•ç† DLQ ä¸­çš„äº‹ä»¶ã€‚

## ğŸ“Š ç›£æ§æŒ‡æ¨™

### é—œéµæŒ‡æ¨™

- **äº‹ä»¶è™•ç†é€Ÿç‡**: events/second
- **è™•ç†å»¶é²**: å¾äº‹ä»¶ç”¢ç”Ÿåˆ°å¯«å…¥è³‡æ–™åº«çš„æ™‚é–“
- **éŒ¯èª¤ç‡**: å¤±æ•—äº‹ä»¶ / ç¸½äº‹ä»¶
- **é‡è©¦æ¬¡æ•¸**: å¹³å‡é‡è©¦æ¬¡æ•¸
- **DLQ å¤§å°**: Dead Letter Queue ä¸­çš„äº‹ä»¶æ•¸
- **è³‡æ–™åº«é€£ç·š**: ä½¿ç”¨ä¸­çš„é€£ç·šæ•¸

### å¥åº·æª¢æŸ¥

```
GET /health

Response 200:
{
  "status": "healthy",
  "kafka": {
    "connected": true,
    "lag": 0  // Consumer lag
  },
  "database": {
    "connected": true,
    "activeConnections": 5,
    "poolSize": 20
  },
  "redis": {
    "connected": true
  },
  "lastProcessedEvent": "2024-01-01T00:00:00.000Z"
}
```

## ğŸ§ª æ¸¬è©¦

```bash
# å–®å…ƒæ¸¬è©¦
nx test db-writer-service

# æ•´åˆæ¸¬è©¦ï¼ˆéœ€è¦ Kafka å’Œ PostgreSQLï¼‰
nx test db-writer-service --testPathPattern=integration

# è¦†è“‹ç‡å ±å‘Š
nx test db-writer-service --coverage
```

### æ¸¬è©¦äº‹ä»¶æŠ•é

```bash
# ä½¿ç”¨ Kafka å·¥å…·æŠ•éæ¸¬è©¦äº‹ä»¶
kafka-console-producer --broker-list localhost:9092 --topic user.created
> {"eventId":"test-123","eventType":"user.created","timestamp":"2024-01-01T00:00:00.000Z","data":{...}}
```

## ğŸ“š ç›¸é—œæ–‡æª”

- [æœå‹™ç¸½è¦½](../../docs/architecture/SERVICES_OVERVIEW.md)
- [CQRS æ¨¡å¼](../../docs/architecture/ADR-001-Pre-Launch-Architecture-Review.md)
- [è³‡æ–™åº«æ¶æ§‹](../../libs/database/README.md)

## ğŸ¤ ä¾è³´æœå‹™

- **PostgreSQL**: å”¯ä¸€å¯«å…¥ç›®æ¨™
- **Kafka**: äº‹ä»¶ä¾†æº
- **Redis**: å†ªç­‰æ€§æª¢æŸ¥

## ğŸš¨ å·²çŸ¥å•é¡Œ

- Dead Letter Queue è™•ç†ä»‹é¢å¾…é–‹ç™¼
- æ‰¹æ¬¡å¯«å…¥å¤±æ•—æ™‚çš„éƒ¨åˆ†å›æ»¾æ©Ÿåˆ¶å¾…å®Œå–„
- è³‡æ–™åº«åˆ†ç‰‡ç­–ç•¥å°šæœªå¯¦ä½œ
- è·¨äº‹ä»¶çš„äº‹å‹™ä¸€è‡´æ€§ä¿è­‰æœ‰é™

è«‹åƒè€ƒ [BUSINESS_LOGIC_GAPS.md](../../docs/BUSINESS_LOGIC_GAPS.md#db-writer-service)ã€‚

## ğŸ“ é–‹ç™¼æ³¨æ„äº‹é …

1. **å”¯ä¸€å¯«å…¥è€…**: çµ•å°ä¸è¦åœ¨å…¶ä»–æœå‹™ä¸­ç›´æ¥å¯«å…¥ PostgreSQL
2. **äº‹ä»¶é †åº**: åŒä¸€å¯¦é«”çš„äº‹ä»¶éœ€ä¿è­‰é †åºï¼ˆä½¿ç”¨ Kafka Partition Keyï¼‰
3. **Schema é·ç§»**: è³‡æ–™åº« Schema è®Šæ›´éœ€è€ƒæ…®å‘å¾Œç›¸å®¹æ€§
4. **ç›£æ§å»¶é²**: ç›£æ§ Kafka Consumer Lagï¼Œç¢ºä¿ä¸ç´¯ç©
5. **å®¹é‡è¦åŠƒ**: æ ¹æ“šäº‹ä»¶é »ç‡è¦åŠƒè³‡æ–™åº«å¯«å…¥å®¹é‡
6. **å‚™ä»½ç­–ç•¥**: å®šæœŸå‚™ä»½è³‡æ–™åº«ï¼Œç¢ºä¿è³‡æ–™å®‰å…¨
7. **æ¸¬è©¦è¦†è“‹**: æ¯ç¨®äº‹ä»¶é¡å‹éƒ½éœ€è¦æ¸¬è©¦æ¡ˆä¾‹

## ğŸ¯ æ“´å±•æ€§

### æ°´å¹³æ“´å±•

- å¢åŠ  Consumer Group çš„ Consumer æ•¸é‡
- Kafka Topic çš„ Partition æ•¸é‡éœ€ >= Consumer æ•¸é‡

### å‚ç›´æ“´å±•

- å¢åŠ è³‡æ–™åº«é€£ç·šæ± å¤§å°
- æå‡æ‰¹æ¬¡è™•ç†å¤§å°

### è³‡æ–™åº«åˆ†ç‰‡

æœªä¾†å¯æ ¹æ“š `userId` é€²è¡Œåˆ†ç‰‡ï¼Œåˆ†æ•£å¯«å…¥å£“åŠ›ã€‚
